{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Ouedraogo Clovis' -u -d -v -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from joblib import dump\n",
    "from collections import Counter\n",
    "from skmultilearn.model_selection.measures import get_combination_wise_output_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from aqosd_experiments.config import *\n",
    "from aqosd_experiments.data import *\n",
    "from aqosd_experiments.utils import *\n",
    "from aqosd_experiments.plot import *\n",
    "from aqosd_experiments.scorers import *\n",
    "from osms import OverheadSensitiveMetricSelection\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "fig = plt_long_stats(RAW_DATASET_PATH,  HOST_LIST)\n",
    "if save:\n",
    "    fig.savefig(FIG_PATH + \"Sample.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-13 18:14:37 2020-12-14 21:16:59\n",
      "Shape of metrics :  (16321, 104) \t Shape of bottlenecks :  (16321, 32)\n",
      "Label cardinality = 2.02984 \t Label density = 0.06343\n"
     ]
    }
   ],
   "source": [
    "metrics, bottlenecks = import_and_prepare_data(RAW_DATASET_PATH,  HOST_LIST)\n",
    "print('Shape of metrics : ',metrics.shape,'\\t','Shape of bottlenecks : ',bottlenecks.shape)\n",
    "print('Label cardinality = %.5f \\t Label density = %.5f' % (bottlenecks.sum(axis=1).mean(),bottlenecks.mean(axis=1).mean()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "fig = plt_all_data(metrics)\n",
    "if save:\n",
    "    fig.savefig(FIG_PATH + \"monitored.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SRV./: Free inodes in %', 'SRV./: Space utilization', 'SRV./: Used space', 'SRV./boot: Free inodes in %', 'SRV./boot: Space utilization', 'SRV./boot: Used space', 'SRV.Available memory', 'SRV.Available memory in %', 'SRV.CPU idle time', 'SRV.CPU iowait time', 'SRV.CPU softirq time', 'SRV.CPU system time', 'SRV.CPU user time', 'SRV.CPU utilization', 'SRV.Context switches per second', 'SRV.Free swap space', 'SRV.Free swap space in %', 'SRV.Interface enp0s8: Bits received', 'SRV.Interface enp0s8: Bits sent', 'SRV.Interrupts per second', 'SRV.Load average (15m avg)', 'SRV.Load average (1m avg)', 'SRV.Load average (5m avg)', 'SRV.Memory utilization', 'SRV.Number of processes', 'SRV.Number of running processes', 'GW1./: Free inodes in %', 'GW1./: Space utilization', 'GW1./: Used space', 'GW1./boot: Free inodes in %', 'GW1./boot: Space utilization', 'GW1./boot: Used space', 'GW1.Available memory', 'GW1.Available memory in %', 'GW1.CPU idle time', 'GW1.CPU iowait time', 'GW1.CPU softirq time', 'GW1.CPU system time', 'GW1.CPU user time', 'GW1.CPU utilization', 'GW1.Context switches per second', 'GW1.Free swap space', 'GW1.Free swap space in %', 'GW1.Interface enp0s8: Bits received', 'GW1.Interface enp0s8: Bits sent', 'GW1.Interrupts per second', 'GW1.Load average (15m avg)', 'GW1.Load average (1m avg)', 'GW1.Load average (5m avg)', 'GW1.Memory utilization', 'GW1.Number of processes', 'GW1.Number of running processes', 'GW11./: Free inodes in %', 'GW11./: Space utilization', 'GW11./: Used space', 'GW11./boot: Free inodes in %', 'GW11./boot: Space utilization', 'GW11./boot: Used space', 'GW11.Available memory', 'GW11.Available memory in %', 'GW11.CPU idle time', 'GW11.CPU iowait time', 'GW11.CPU softirq time', 'GW11.CPU system time', 'GW11.CPU user time', 'GW11.CPU utilization', 'GW11.Context switches per second', 'GW11.Free swap space', 'GW11.Free swap space in %', 'GW11.Interface enp0s8: Bits received', 'GW11.Interface enp0s8: Bits sent', 'GW11.Interrupts per second', 'GW11.Load average (15m avg)', 'GW11.Load average (1m avg)', 'GW11.Load average (5m avg)', 'GW11.Memory utilization', 'GW11.Number of processes', 'GW11.Number of running processes', 'GW111./: Free inodes in %', 'GW111./: Space utilization', 'GW111./: Used space', 'GW111./boot: Free inodes in %', 'GW111./boot: Space utilization', 'GW111./boot: Used space', 'GW111.Available memory', 'GW111.Available memory in %', 'GW111.CPU idle time', 'GW111.CPU iowait time', 'GW111.CPU softirq time', 'GW111.CPU system time', 'GW111.CPU user time', 'GW111.CPU utilization', 'GW111.Context switches per second', 'GW111.Free swap space', 'GW111.Free swap space in %', 'GW111.Interface enp0s8: Bits received', 'GW111.Interface enp0s8: Bits sent', 'GW111.Interrupts per second', 'GW111.Load average (15m avg)', 'GW111.Load average (1m avg)', 'GW111.Load average (5m avg)', 'GW111.Memory utilization', 'GW111.Number of processes', 'GW111.Number of running processes']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "['GW1.cpu', 'GW1.diskio', 'GW1.diskspace', 'GW1.memory', 'GW1.network delay', 'GW1.network packet corrupt', 'GW1.network packet duplicate', 'GW1.network packet loss', 'GW11.cpu', 'GW11.diskio', 'GW11.diskspace', 'GW11.memory', 'GW11.network delay', 'GW11.network packet corrupt', 'GW11.network packet duplicate', 'GW11.network packet loss', 'GW111.cpu', 'GW111.diskio', 'GW111.diskspace', 'GW111.memory', 'GW111.network delay', 'GW111.network packet corrupt', 'GW111.network packet duplicate', 'GW111.network packet loss', 'SRV.cpu', 'SRV.diskio', 'SRV.diskspace', 'SRV.memory', 'SRV.network delay', 'SRV.network packet corrupt', 'SRV.network packet duplicate', 'SRV.network packet loss']\n"
     ]
    }
   ],
   "source": [
    "metric_names, bottleneck_names = list(metrics.columns), list(bottlenecks.columns)\n",
    "print(metric_names)\n",
    "print(100*'-')\n",
    "print(bottleneck_names)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = plot_number_of_instance(bottlenecks)\n",
    "if save:\n",
    "    fig.savefig(FIG_PATH + \"Number_of_instance_with_a_class_label.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "fig = plt_corr_metrics(metrics, c3='w', method='spearman')\n",
    "if save:\n",
    "    fig.savefig(FIG_PATH + \"Correlation in Metrics.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "fig = plt_corr_bottlenecks(bottlenecks, c3='w', method='spearman')\n",
    "if save:\n",
    "    fig.savefig(FIG_PATH + \"Correlation in Bottlenecks.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = scale_metrics(metrics, StandardScaler()) #MinMaxScaler\n",
    "train_indexes, test_indexes = next(CV_2.split(metrics, bottlenecks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13060, 104), (13060, 32), (3261, 104), (3261, 32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = metrics.iloc[train_indexes, :], bottlenecks.iloc[train_indexes, :]\n",
    "X_test, y_test = metrics.iloc[test_indexes, :], bottlenecks.iloc[test_indexes, :]\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13060, 104), (13060, 32), (3261, 104), (3261, 32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = X_train.values, y_train.values,  X_test.values, y_test.values\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(19, 19)</th>\n",
       "      <th>(1, 1)</th>\n",
       "      <th>(1, 19)</th>\n",
       "      <th>(3, 19)</th>\n",
       "      <th>(3, 3)</th>\n",
       "      <th>(30, 30)</th>\n",
       "      <th>(3, 30)</th>\n",
       "      <th>(11, 30)</th>\n",
       "      <th>(11, 11)</th>\n",
       "      <th>(4, 4)</th>\n",
       "      <th>...</th>\n",
       "      <th>(1, 23)</th>\n",
       "      <th>(16, 28)</th>\n",
       "      <th>(14, 28)</th>\n",
       "      <th>(5, 24)</th>\n",
       "      <th>(0, 23)</th>\n",
       "      <th>(5, 26)</th>\n",
       "      <th>(1, 7)</th>\n",
       "      <th>(13, 30)</th>\n",
       "      <th>(6, 30)</th>\n",
       "      <th>(7, 31)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>1020.0</td>\n",
       "      <td>737.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>954.0</td>\n",
       "      <td>721.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1168.0</td>\n",
       "      <td>860.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>254.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       (19, 19)  (1, 1)  (1, 19)  (3, 19)  (3, 3)  (30, 30)  (3, 30)  \\\n",
       "train    1020.0   737.0     58.0     47.0   954.0     721.0     79.0   \n",
       "test      254.0   184.0     15.0     11.0   242.0     180.0     20.0   \n",
       "\n",
       "       (11, 30)  (11, 11)  (4, 4)  ...  (1, 23)  (16, 28)  (14, 28)  (5, 24)  \\\n",
       "train      61.0    1168.0   860.0  ...     24.0      29.0       8.0     16.0   \n",
       "test       15.0     292.0   215.0  ...      6.0       7.0       2.0      4.0   \n",
       "\n",
       "       (0, 23)  (5, 26)  (1, 7)  (13, 30)  (6, 30)  (7, 31)  \n",
       "train      9.0     11.0    15.0      18.0      5.0     10.0  \n",
       "test       3.0      2.0     4.0       5.0      1.0      3.0  \n",
       "\n",
       "[2 rows x 502 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'train': Counter(str(c) for row in get_combination_wise_output_matrix(y_train, order=2) for c in row),\n",
    "    'test' : Counter(str(c) for row in get_combination_wise_output_matrix(y_test, order=2) for c in row)\n",
    "}).T.fillna(0.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results = {}\n",
    "def train_and_plot(X_train, y_train):\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes= iter(axes.flat)\n",
    "    for clf_name, clf in CLASSIFIERS.items():\n",
    "        print(80*'-')\n",
    "        print('#',clf_name.ljust(20), end=' ')\n",
    "        start=time.time()\n",
    "        clf.fit(X_train, y_train)\n",
    "        tt= time.time()-start\n",
    "        if tt> 60 :\n",
    "            print('>','time:',tt/60,'minutes')\n",
    "        else:\n",
    "            print('>','time:',tt,'secondes')\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_pred_proba = clf.predict_proba(X_test)\n",
    "        if not hasattr(y_pred, 'toarray'):\n",
    "            y_pred = sparse.csr_matrix(y_pred)\n",
    "        print_metrics(y_test, y_pred.toarray())\n",
    "        plt_roc_auc(next(axes),clf_name, y_pred_proba, y_test, bottleneck_names)\n",
    "        results[clf_name] = y_pred\n",
    "        df = compute_precision_sensitivity_specificity(y_test, y_pred,bottleneck_names)\n",
    "        if save:\n",
    "            df.to_csv(MODELS_PATH + clf_name + '_precision_sensitivity_specificity.csv', index=True)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "fig = train_and_plot(X_train, y_train)\n",
    "if save:\n",
    "    fig.savefig(FIG_PATH + \"ROC Curve.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "fig = plot_mcc(results, y_test, bottleneck_names)\n",
    "if save:\n",
    "    fig.savefig(FIG_PATH + \"MCC.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_c = compute_classification_score(results, y_test, many=True)\n",
    "if save:\n",
    "    df_c.to_csv(MODELS_PATH + 'classification_score.csv', index=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_r = compute_multilabel_ranking_metrics(results, y_test)\n",
    "if save:\n",
    "    df_r.to_csv(MODELS_PATH + 'multilabel_ranking_metrics.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute_measure_per_label(results, y_test, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#fig = perf_viz(results, y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "best={'Accuracy':df_c['Accuracy'].idxmax(),'Precision':df_c['Precision'].idxmax(),'Recall':df_c['Recall'].idxmax(),'F1':df_c['F1'].idxmax(),'Hamming loss':df_c['Hamming loss'].idxmin(),'Jaccard index':df_c['Jaccard index'].idxmax(),'Coverage error':df_r['Coverage error'].idxmin(), 'Label ranking average precision':df_r['Label ranking average precision'].idxmax(),'Label ranking loss':df_r['Label ranking loss'].idxmin()}\n",
    "best={key: [value for value, check_key in best.items() if check_key==key] for key in best.values()}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for k in best:\n",
    "    print(k, ':', best[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection=\"ML-kNN\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "y_pred = results[selection]\n",
    "fig = plot_multi_confusion_matrix(y_pred,y_test, bottleneck_names, selection)\n",
    "if save:\n",
    "    fig.savefig(FIG_PATH + \"confusion_matrix.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clf=CLASSIFIERS[selection]\n",
    "print('#',selection, end=' ')\n",
    "start=time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "print('>','time:',(time.time()-start)//60+1,'minutes')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# test prediction\n",
    "from scipy import sparse\n",
    "def test_predic(pick, clf):\n",
    "    test_data = X_test[pick:pick+1,:]\n",
    "    true_data = y_test[pick]\n",
    "    prediction_data= clf.predict(test_data)\n",
    "    if not hasattr(prediction_data, 'toarray'):\n",
    "            prediction_data = sparse.csr_matrix(prediction_data)\n",
    "    prediction_data=prediction_data.toarray()[0]\n",
    "    true_data_class = [bottleneck_names[i] for i in range(len(bottleneck_names)) if true_data[i]>0]  \n",
    "    prediction_data_class = [bottleneck_names[i] for i in range(len(bottleneck_names)) if prediction_data[i]>0]\n",
    "    print('true class =', true_data_class, '| num. of labels =',len(true_data_class))\n",
    "    print('pred class=', prediction_data_class, '| num. of labels =',len(prediction_data_class))\n",
    "pick=40\n",
    "test_predic(pick, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_metric : { SRV :  26 , GW1 :  26 , GW11 :  26 , GW111 :  26 }\n"
     ]
    }
   ],
   "source": [
    "n_metric_SRV = sum(1 for s in metric_names if 'SRV.' in s)\n",
    "n_metric_GW1 = sum(1 for s in metric_names if 'GW1.' in s)\n",
    "n_metric_GW11 = sum(1 for s in metric_names if 'GW11.' in s)\n",
    "n_metric_GW111 = sum(1 for s in metric_names if 'GW111.' in s)\n",
    "print(\"n_metric : { SRV : \",n_metric_SRV,\", GW1 : \",n_metric_GW1, \", GW11 : \",n_metric_GW11,\", GW111 : \",n_metric_GW111,\"}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Scénario 0 : \n",
    "    Overhead budget : infinie\n",
    "    Metrics Costs: 1\n",
    "Scénario 1 : \n",
    "    Overhead budget : 50%\n",
    "    Metrics Costs: 1\n",
    "Scénario 2 : \n",
    "    Overhead budget : 25%\n",
    "    Metrics Costs: 1\n",
    "Scénario 3 : \n",
    "    Overhead budget : 50%\n",
    "    Metrics Costs: SRV = 0.5 , GW1 = 1, GW11=1.5, GW111= 2\n",
    "Scénario 4 : \n",
    "    Overhead budget : 25%\n",
    "    Metrics Costs: SRV = 0.5 , GW1 = 1, GW11=1.5, GW111= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 4 : Limited budget (1/4 total overhead) + Overhead increases by a factor of 0.5 from SRV --> GW111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "scn_name='scn_4'\n",
    "SRV_costs= 0.5 * np.ones(n_metric_SRV)\n",
    "GW1_costs= 1.0 * np.ones(n_metric_GW1)\n",
    "GW11_costs= 1.5 * np.ones(n_metric_GW11)\n",
    "GW111_costs= 2.0 * np.ones(n_metric_GW111)          \n",
    "overheads=np.concatenate([SRV_costs, GW1_costs, GW11_costs, GW111_costs])\n",
    "overhead_budget=np.sum(overheads)//4\n",
    "best_clf=CLASSIFIERS[selection]\n",
    "osdms = OverheadSensitiveMetricSelection(best_clf, overheads=overheads, overhead_budget=overhead_budget, \n",
    "                                         scoring=SCORING, verbose=2, cv=CV, n_jobs=-1)\n",
    "start=time.time()\n",
    "osdms.fit(metrics.values, bottlenecks.values, user_metric_names=metric_names)\n",
    "print(5*'-'+'>','time:',(time.time()-start)//60+1,'minutes')\n",
    "if save:\n",
    "    dump(osdms, MODELS_PATH + scn_name + 'OSDMS.joblib')\n",
    "print('best combination (Score: %.5f, numb : %d):\\n%s' % (osdms.k_score_,len(osdms.k_metric_names_),\n",
    "                                                           osdms.k_metric_names_))\n",
    "fig, df = plot_osdm(osdms)\n",
    "if save:\n",
    "    df.to_csv(MODELS_PATH + scn_name + '_metric_selection.csv', index=True)\n",
    "    fig.savefig(FIG_PATH + scn_name + '_metric_selection.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 3 : Limited budget (1/2 total overhead) + Overhead increases by a factor of 0.5 from SRV --> GW111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scn_name='scn_3'\n",
    "SRV_costs= 0.5 * np.ones(n_metric_SRV)\n",
    "GW1_costs= 1.0 * np.ones(n_metric_GW1)\n",
    "GW11_costs= 1.5 * np.ones(n_metric_GW11)\n",
    "GW111_costs= 2.0 * np.ones(n_metric_GW111)          \n",
    "overheads=np.concatenate([SRV_costs, GW1_costs, GW11_costs, GW111_costs])\n",
    "overhead_budget=np.sum(overheads)//2\n",
    "best_clf=CLASSIFIERS[selection]\n",
    "osdms = OverheadSensitiveMetricSelection(best_clf, overheads=overheads, overhead_budget=overhead_budget, \n",
    "                                         scoring=SCORING, verbose=2, cv=CV, n_jobs=-1)\n",
    "start=time.time()\n",
    "osdms.fit(metrics.values, bottlenecks.values, user_metric_names=metric_names)\n",
    "print(5*'-'+'>','time:',(time.time()-start)//60+1,'minutes')\n",
    "if save:\n",
    "    dump(osdms, MODELS_PATH + scn_name + 'OSDMS.joblib')\n",
    "print('best combination (Score: %.5f, numb : %d):\\n%s' % (osdms.k_score_,len(osdms.k_metric_names_),\n",
    "                                                           osdms.k_metric_names_))\n",
    "fig, df = plot_osdm(osdms)\n",
    "if save:\n",
    "    df.to_csv(MODELS_PATH + scn_name + '_metric_selection.csv', index=True)\n",
    "    fig.savefig(FIG_PATH + scn_name + '_metric_selection.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2 : Limited budget (1/4 total overhead) + Same overhead for all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scn_name='scn_2'\n",
    "SRV_costs= 1.0 * np.ones(n_metric_SRV)\n",
    "GW1_costs= 1.0 * np.ones(n_metric_GW1)\n",
    "GW11_costs= 1.0 * np.ones(n_metric_GW11)\n",
    "GW111_costs= 1.0 * np.ones(n_metric_GW111)          \n",
    "overheads=np.concatenate([SRV_costs, GW1_costs, GW11_costs, GW111_costs])\n",
    "overhead_budget=np.sum(overheads)//4\n",
    "bbest_clf=CLASSIFIERS[selection]\n",
    "osdms = OverheadSensitiveMetricSelection(best_clf, overheads=overheads, overhead_budget=overhead_budget, \n",
    "                                         scoring=SCORING, verbose=2, cv=CV, n_jobs=-1)\n",
    "start=time.time()\n",
    "osdms.fit(metrics.values, bottlenecks.values, user_metric_names=metric_names)\n",
    "print(5*'-'+'>','time:',(time.time()-start)//60+1,'minutes')\n",
    "if save:\n",
    "    dump(osdms, MODELS_PATH + scn_name + 'OSDMS.joblib')\n",
    "print('best combination (Score: %.5f, numb : %d):\\n%s' % (osdms.k_score_,len(osdms.k_metric_names_),\n",
    "                                                           osdms.k_metric_names_))\n",
    "fig, df = plot_osdm(osdms)\n",
    "if save:\n",
    "    df.to_csv(MODELS_PATH + scn_name + '_metric_selection.csv', index=True)\n",
    "    fig.savefig(FIG_PATH + scn_name + '_metric_selection.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Scenario 1 : Limited budget (1/2 total overhead) + Same overhead for all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scn_name='scn_1'\n",
    "SRV_costs= 1.0 * np.ones(n_metric_SRV)\n",
    "GW1_costs= 1.0 * np.ones(n_metric_GW1)\n",
    "GW11_costs= 1.0 * np.ones(n_metric_GW11)\n",
    "GW111_costs= 1.0 * np.ones(n_metric_GW111)          \n",
    "overheads=np.concatenate([SRV_costs, GW1_costs, GW11_costs, GW111_costs])\n",
    "overhead_budget=np.sum(overheads)//2\n",
    "best_clf=CLASSIFIERS[selection]\n",
    "osdms = OverheadSensitiveMetricSelection(best_clf, overheads=overheads, overhead_budget=overhead_budget, \n",
    "                                         scoring=SCORING, verbose=2, cv=CV, n_jobs=-1)\n",
    "start=time.time()\n",
    "osdms.fit(metrics.values, bottlenecks.values, user_metric_names=metric_names)\n",
    "print(5*'-'+'>','time:',(time.time()-start)//60+1,'minutes')\n",
    "if save:\n",
    "    dump(osdms, MODELS_PATH + scn_name + 'OSDMS.joblib')\n",
    "print('best combination (Score: %.5f, numb : %d):\\n%s' % (osdms.k_score_,len(osdms.k_metric_names_),\n",
    "                                                           osdms.k_metric_names_))\n",
    "fig, df = plot_osdm(osdms)\n",
    "if save:\n",
    "    df.to_csv(MODELS_PATH + scn_name + '_metric_selection.csv', index=True)\n",
    "    fig.savefig(FIG_PATH + scn_name + '_metric_selection.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 0 : Unlimited budget + Same overhead for all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scn_name='scn_0'\n",
    "SRV_costs= 1.0 * np.ones(n_metric_SRV)\n",
    "GW1_costs= 1.0 * np.ones(n_metric_GW1)\n",
    "GW11_costs= 1.0 * np.ones(n_metric_GW11)\n",
    "GW111_costs= 1.0 * np.ones(n_metric_GW111)          \n",
    "overheads=np.concatenate([SRV_costs, GW1_costs, GW11_costs, GW111_costs])\n",
    "overhead_budget=np.sum(overheads)\n",
    "best_clf=CLASSIFIERS[selection]\n",
    "osdms = OverheadSensitiveMetricSelection(best_clf, overheads=overheads, overhead_budget=overhead_budget, \n",
    "                                         scoring=SCORING, verbose=2, test_indexes=test_indexes, n_jobs=-1)\n",
    "start=time.time()\n",
    "osdms.fit(metrics.values, bottlenecks.values, user_metric_names=metric_names)\n",
    "print(5*'-'+'>','time:',(time.time()-start)//60+1,'minutes')\n",
    "if save:\n",
    "    dump(osdms, MODELS_PATH + scn_name + 'OSDMS.joblib')\n",
    "print('best combination (Score: %.5f, numb : %d):\\n%s' % (osdms.k_score_,len(osdms.k_metric_names_),\n",
    "                                                           osdms.k_metric_names_))\n",
    "fig, df = plot_osdm(osdms)\n",
    "if save:\n",
    "    df.to_csv(MODELS_PATH + scn_name + '_metric_selection.csv', index=True)\n",
    "    fig.savefig(FIG_PATH + scn_name + '_metric_selection.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
